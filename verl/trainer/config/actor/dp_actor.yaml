# Format checks enforced on CI:
# 1. Comments must appear above each field.
# 2. There must be a blank line between each field.
# 3. Inline comments (after a field on the same line) are not allowed.
# 4. Indentation level is respected for nested fields.

# defaults specify the default config from each component
defaults:

  # fsdp optimizer config
  - ../optim@optim: fsdp

  # fsdp engine config
  - ../engine@fsdp_config: fsdp

  # dp actor config, inheriting from trainer/config/actor/actor.yaml
  - actor

  # load the reference default config, then apply the fields in the current yaml
  - _self_

# Target class for this configuration
_target_: verl.workers.config.FSDPActorConfig

# TODO(haibin.lin): switch to fsdp2
strategy: fsdp

# Gradient clipping for actor updates, specific to the strategy.
grad_clip: 1.0

# Sequence parallelism size for Ulysses-style model parallelism
# oc.select: the default val for ref.ulysses_sequence_parallel_size
# [DEPRECATED] use fsdp_config.ulysses_sequence_parallel_size instead
ulysses_sequence_parallel_size: 1

# calculate entropy with chunking to reduce memory peak
entropy_from_logits_with_chunking: False

# recompute entropy
entropy_checkpointing: False

# Whether to remove padding tokens in inputs during training
use_remove_padding: ${oc.select:actor_rollout_ref.model.use_remove_padding,false}

# This computes Σπ² needed for the Logit-Gradient Norm proxy W(τ) = Σ_t[1 - 2π_t + Σπ²]
# c.f. https://yingru.notion.site/The-Optimal-Token-Baseline-399211a558b782cfa936014c0d42dfb8
calculate_sum_pi_squared: False

# Enable gradient checkpointing for sum_pi_squared computation (saves memory)
sum_pi_squared_checkpointing: False

# QAT (Quantization-Aware Training) configuration
# When enabled:
#   - QAT is automatically applied to actor model during training
#   - Fused scales (QKV/GateUp) are automatically enabled for training-inference consistency
#   - Fast quantization is used when syncing weights to vLLM rollout
# Supported modes: "w4a16" (NVFP4 weight-only)
# Note: "w4a4" mode is included in the code but currently has KL divergence issues and is NOT recommended for use.
# For usage examples, see: https://github.com/verl-project/verl-recipe/blob/main/qat/README.md
qat:

  # Whether to enable QAT
  enable: false

  # Quantization mode: "w4a16" (weight-only). "w4a4" is experimental and not recommended.
  mode: "w4a16"

  # Quantization group size (NVFP4 requires 16)
  group_size: 16

  # Patterns to ignore (e.g., lm_head, embed_tokens)
  ignore_patterns:

    - "lm_head"
    - "embed_tokens"
    - "re:.*mlp.gate$"

  # Activation observer for W4A4 mode: "static_minmax", "memoryless_minmax", or "minmax"
  activation_observer: "static_minmax"

  # Path to vLLM quantization config JSON file
  quantization_config_path: null
